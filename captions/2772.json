[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.125, 0.15536821350570798, 0.8941326530612245, 0.31092436974789917], "Caption": "Fig. 1. cite2vec enables exploration of document collections via citation contexts: how people tend to use documents. Our interface enables document usage exploration via user-prescribed concepts: here we show a 2D projection of words, where each word is composed with the specified phrase \u201cobject detection\u201d (left). Each document \u2013 shown as a disk \u2013 is positioned near words whose phrase compositions best describe its usage, here taken as \u201dcrowd object detection\u201d. The user can add phrases (right) to compare usage, resulting in new document projections, with the highlighted document taking on the more precise usage \u201dpedestrian trackers\u201d.", "DPI": 100, "CaptionBB": [74, 345, 724, 413], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 3, "Type": "Figure", "Imagebbox": [0.05229591836734694, 0.05042016806722689, 0.9655612244897959, 0.31932773109243695], "Caption": "Fig. 2. Overview of cite2vec: from a given document collection we resolve all document citations to unique identifiers, and jointly learn a semantic embedding of words and documents. We then perform a 2D projection of the words and documents from the embedding, and allow the steering of document projections via user-defined concepts. Here, the user specifies \u201cface recognition\u201d, resulting in each word being composed with this provided phrase. Upon specifying \u201cemotion\u201d we observe a document change its projection due to a better word-phrase composition.", "DPI": 100, "CaptionBB": [41, 355, 756, 410], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 4, "Type": "Figure", "Imagebbox": [0.05229591836734694, 0.05304263211035913, 0.4639830508474576, 0.3641456582633053], "Caption": "Fig. 3. Illustration of the Skip-gram model for citations. The document word is far from other words randomly drawn from some distribution P, denoted by PID is referenced by two different documents (top and middle), a process known as negative sampling. The distribution P is typically highlighted by its citation contexts in both documents. These contextual taken as the unigram distribution over Vw (normalized word counts), words become associated with the document in the embedding space raised to a certain power, 34 in [30]. The objective of the Skip-gram", "DPI": 100, "CaptionBB": [27, 389, 743, 453], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 4, "Type": "Figure", "Imagebbox": [0.5021186440677966, 0.051492244513459906, 0.9502551020408163, 0.17086834733893558], "Caption": "Fig. 4. We show unigram distributions for words (left) and documents (right), sorted by frequencies, showing representative words and docu- ments from their vocabularies of size 44,752 and 18,832, respectively. The discrepancies between their distributions necessitate a different negative sampling scheme in the skip-gram model.", "DPI": 100, "CaptionBB": [392, 196, 744, 264], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 5, "Type": "Figure", "Imagebbox": [0.559322033898305, 0.05042016806722689, 0.923469387755102, 0.20261437908496732], "Caption": "Fig. 5. We show clusters of words associated with the learned cite2vec embedding. Note the diversity in the clusters: some refer to standard computer vision topics, while others are more general semantic concepts like animals, climate, and funding acknowledgments.", "DPI": 100, "CaptionBB": [405, 231, 755, 286], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 5, "Type": "Figure", "Imagebbox": [0.5169491525423728, 0.7243604615677235, 0.7415254237288136, 0.8902519344359405], "Caption": "Fig. 5. We show clusters of words associated with the learned cite2vec embedding. Note the diversity in the clusters: some refer to standard computer vision topics, while others are more general semantic concepts like animals, climate, and funding acknowledgments.", "DPI": 100, "CaptionBB": [405, 231, 755, 286], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 6, "Type": "Figure", "Imagebbox": [0.5211864406779662, 0.05304263211035913, 0.9279661016949152, 0.27170868347338933], "Caption": "Fig. 7. Documents-as-concepts: the user prescribes all projected words to be composed with the selected document (shown in the upper right), and all documents are positioned to satisfy the concept composition.", "DPI": 100, "CaptionBB": [393, 303, 744, 345], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 7, "Type": "Figure", "Imagebbox": [0.5382653061224489, 0.05415499533146592, 0.9426020408163265, 0.28291316526610644], "Caption": "Fig. 10. Analogy concepts: the user may specify the first part of a document:word analogy (shown on top), resulting in a new document projection such that a document\u2019s proximity to a word indicates their likely completion of the analogy.", "DPI": 100, "CaptionBB": [405, 316, 755, 371], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 8, "Page": 7, "Type": "Figure", "Imagebbox": [0.07397959183673469, 0.05322128851540616, 0.47831632653061223, 0.2735760971055089], "Caption": "Fig. 8. Comparing concepts: the user specifies three coarse-level con- cepts, enabling them to observe distinct themes based on the document distribution and document assignment to concepts, while also discovering words that share concepts.", "DPI": 100, "CaptionBB": [40, 306, 392, 361], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 9, "Page": 7, "Type": "Figure", "Imagebbox": [0.5169491525423728, 0.5584689886995065, 0.6991525423728814, 0.6282364305599716], "Caption": "Fig. 10. Analogy concepts: the user may specify the first part of a document:word analogy (shown on top), resulting in a new document projection such that a document\u2019s proximity to a word indicates their likely completion of the analogy.", "DPI": 100, "CaptionBB": [405, 316, 755, 371], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 10, "Page": 8, "Type": "Figure", "Imagebbox": [0.05739795918367347, 0.4024276377217554, 0.4639830508474576, 0.6375387561413669], "Caption": "In this case study we have a fellow researcher, whose expertise is in ma- Fig. 12. We show how a non-technical phrase, user interaction, leads to chine learning, use our system to research an unfamiliar subdiscipline: different techniques that use interaction as part of their approach, here weakly supervised learning techniques in computer vision. The user is highlighting interactive segmentation techniques.", "DPI": 100, "CaptionBB": [28, 685, 745, 730], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 11, "Page": 8, "Type": "Figure", "Imagebbox": [0.04025423728813559, 0.054593019707258356, 0.9449152542372882, 0.3127917833800187], "Caption": "Fig. 11. We show use cases for exploring document usage via multiple phrase compositions. In (a) we highlight how standard domain-specific phrases lead to basic document usage, namely techniques and data that are used by such documents. In (b) we highlight how the phrase \u201ctheory\u201d leads to papers that are naturally theoretically-based, a concept that would be difficult to discover via the papers themselves, but becomes apparent in observing citation contexts.", "DPI": 100, "CaptionBB": [28, 351, 744, 406], "first_confirmed": false, "second_confirmed": false}]