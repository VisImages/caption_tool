[{"ImageID": 0, "Page": 2, "Type": "Table", "Imagebbox": [0.07233502538071065, 0.7237209302325581, 0.9111675126903553, 0.8511627906976744], "Caption": "Table 1. Are variances in the metaphors of questions asked responsible for the inconsistent performance of a single visualization method across evaluation studies? We looked at several recent tree visualization evaluation papers that both included a treemap in their comparison and published their task questions. For each paper, we counted the number of questions or task descriptions that re\ufb02ect a containment metaphor, i.e., those which used words like contains, has, inside, or within when describing relationships among nodes in the hierarchy. We then ranked each of the methods in the comparison by average response time overall, with 1 being the fastest method. Where exact response time was not reported, we estimated it based on results graphs. This preliminary meta-analysis suggests a possible relationship between metaphor compatibility and response time.", "DPI": 100, "CaptionBB": [29, 939, 743, 1032], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 2, "Type": "Table", "Imagebbox": [0.03505535055350553, 0.06373144193538609, 0.48708487084870844, 0.25266531507978557], "Caption": "Table 1. Are variances in the metaphors of questions asked responsible for the inconsistent performance of a single visualization method across evaluation studies? We looked at several recent tree visualization evaluation papers that both included a treemap in their comparison and published their task questions. For each paper, we counted the number of questions or task descriptions that re\ufb02ect a containment metaphor, i.e., those which used words like contains, has, inside, or within when describing relationships among nodes in the hierarchy. We then ranked each of the methods in the comparison by average response time overall, with 1 being the fastest method. Where exact response time was not reported, we estimated it based on results graphs. This preliminary meta-analysis suggests a possible relationship between metaphor compatibility and response time.", "DPI": 100, "CaptionBB": [29, 939, 743, 1032], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 3, "Type": "Figure", "Imagebbox": [0.06091370558375635, 0.061395348837209304, 0.9543147208121827, 0.31069767441860463], "Caption": "Fig. 2. These screenshots show our study design as it was seen by participants. Each participant viewed one of two types of tree visualization. Using the InfoVis Toolkit, we kept super\ufb01cial qualities of the visualizations (e.g., color, label size, and display size) as constant as possible. The visualizations were not interactive. Participants answered questions about the data by pressing keys on the keyboard; for each of three sessions, participants saw eight questions, four of which were metaphorically compatible with their visualization and four of which were incompatible. The order of compatible and incompatible questions was random. Each session showed a different but similarly complex dataset based on a hypothetical \ufb01le structure.", "DPI": 100, "CaptionBB": [42, 365, 756, 445], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 3, "Type": "Table", "Imagebbox": [0.08375634517766498, 0.42511627906976746, 0.9314720812182741, 0.6325581395348837], "Caption": "Table 2. The eight task questions given to participants in our study, framed in either a metaphor of hierarchy as containment or hierarchy as levels, as asked during the \ufb01rst trial session. For the subsequent sessions, the speci\ufb01c \ufb01les and directories mentioned in the questions were altered to match the dataset being visualized, but the wording remained unchanged. For each session, a participant saw four questions from the Levels list and four from the Containers list, and question order was randomized. These questions are based on common task questions asked in tree visualization evaluation papers.", "DPI": 100, "CaptionBB": [42, 704, 756, 771], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 4, "Type": "Figure", "Imagebbox": [0.4987309644670051, 0.06778002493133752, 0.9467005076142132, 0.3944186046511628], "Caption": "Fig. 4. This shows the paired response differences for each of the eight questions (Table 2) across the \ufb01rst two sessions. For each participant and each question, we take the participant\u2019s response time on an in- compatible version of the question and on a compatible version of the question. These bars show the averages for all participants for each of the eight questions. Since both participant and question are held con- stant, this is the most \ufb01ne-grained way to look at the effect of compati- bility alone. While there is a great deal of variation across questions, the most dramatic differences show a slower response time for incompatible questions.", "DPI": 100, "CaptionBB": [394, 442, 744, 574], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 4, "Type": "Figure", "Imagebbox": [0.03874538745387454, 0.059682858939434674, 0.4723247232472325, 0.402462885929988], "Caption": "Fig. 4. This shows the paired response differences for each of the eight questions (Table 2) across the \ufb01rst two sessions. For each participant and each question, we take the participant\u2019s response time on an in- compatible version of the question and on a compatible version of the question. These bars show the averages for all participants for each of the eight questions. Since both participant and question are held con- stant, this is the most \ufb01ne-grained way to look at the effect of compati- bility alone. While there is a great deal of variation across questions, the most dramatic differences show a slower response time for incompatible questions.", "DPI": 100, "CaptionBB": [394, 442, 744, 574], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 5, "Type": "Figure", "Imagebbox": [0.5279187817258884, 0.06976744186046512, 0.9365482233502538, 0.3767441860465116], "Caption": "Fig. 6. Here we divide the participants into quartiles based on the num- ber of questions they answered correctly (out of 24). Participants in higher-accuracy groups have a much higher tendency to perform faster on metaphorically compatible questions, while those in the lower ac- curacy groups tend to perform faster on incompatible questions. This suggests that an inability to internalize the visual metaphor of a visual- ization is linked to an inability to extract information from the visualization accurately.", "DPI": 100, "CaptionBB": [407, 430, 757, 536], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 5, "Type": "Figure", "Imagebbox": [0.06218274111675127, 0.06976744186046512, 0.4885786802030457, 0.32558139534883723], "Caption": "Fig. 5. We calculate each participant\u2019s preference for compatible metaphors by subtracting their average response time on compatible questions from their average response time on incompatible questions. High positive values therefore indicate a strong preference for compat- ible metaphors, while negative values indicate a preference for incom- patible metaphors. When participants are plotted in terms of their pref- erence and overall accuracy, there is a strong correlation between pref- erence for compatible metaphors and accuracy, r (31) = 0.49, p < 0.01.", "DPI": 100, "CaptionBB": [42, 372, 392, 479], "first_confirmed": false, "second_confirmed": false}]