[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.12022900763358779, 0.1296263966480447, 0.8912213740458015, 0.3277310924369748], "Caption": "Fig. 1: We provide an interactive analysis approach for mobile eye tracking by visualizing a gaze data from different videos in a b cluster view that depicts aggregated information about individual areas of interest. An additional c scarf plot provides a temporal overview of when annotated areas were investigated by the participants.", "DPI": 100, "CaptionBB": [73, 360, 722, 405], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 3, "Type": "Figure", "Imagebbox": [0.05102040816326531, 0.05001745810055866, 0.9655612244897958, 0.20354808590102708], "Caption": "Fig. 2: Analysis process for mobile eye tracking data (red): AOIs can be defined by annotating the video (gray) or the gaze data. We facilitate the annotation by automatic pre-processing of gaze data (yellow) and reduce the annotation to an interactive image labeling task (green). For the analysis of mapped gaze data on AOIs (blue), common visualization techniques such as histograms and scarf plots can be applied.", "DPI": 100, "CaptionBB": [41, 226, 755, 268], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 3, "Type": "Figure", "Imagebbox": [0.5178571428571429, 0.27450980392156865, 0.9642857142857143, 0.41363211951447243], "Caption": "Fig. 3: Segmentation of a thumbnail sequence: Changes in the image sequence lead to similarity values below the threshold ( a , c ) and start a new segment. Smaller changes due to short saccades or head move- ment are aggregated in the same segment b . The first element of a segment is chosen as representative (yellow border).", "DPI": 100, "CaptionBB": [405, 452, 755, 524], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 4, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.051414106145251395, 0.9502551020408163, 0.21288515406162467], "Caption": "Fig. 4: Overview of the video segmentation and image clustering process: Thumbnails are extracted from all videos and temporally aggregated. The segment representatives are compared using a bag-of-features approach. Clustering is then performed on the resulting similarity matrix.", "DPI": 100, "CaptionBB": [28, 236, 742, 265], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 5, "Type": "Figure", "Imagebbox": [0.05102040816326531, 0.05281075418994413, 0.9655612244897958, 0.4696545284780579], "Caption": "Fig. 5: Overview: Our implementation consists of a main view with four different elements: a the cluster view lists all clusters in the data sorted by their accumulated duration. a1 To the left of each cluster representative, the cluster elements are shown. a2 To the right, attention histograms for the clusters are shown. a3 The total attention of the labeled clusters is shown in the histogram on the left. b The timeline overview at the top shows the clusters that are viewed by the majority of the participants. c The scarf plot at the bottom displays for each participant over time which cluster was investigated. d A tooltip shows additional information when hovering over scarf plot segments.", "DPI": 100, "CaptionBB": [40, 511, 754, 594], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 6, "Type": "Figure", "Imagebbox": [0.4987244897959184, 0.05001745810055866, 0.9502551020408163, 0.31746031746031744], "Caption": "Fig. 7: Video player. With the video player, recordings of individual participants can be viewed. a Their gaze positions are marked with a yellow square and bounding box. b A polygonal area can be marked (red) in the video frame to search for all similar looking thumbnails.", "DPI": 100, "CaptionBB": [393, 348, 743, 407], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 6, "Type": "Figure", "Imagebbox": [0.03443877551020408, 0.05001745810055866, 0.48282442748091603, 0.30065359477124187], "Caption": "Fig. 6: Cluster editor. The cluster editor allows editing, merging, and deleting individual clusters. It shows a list of all clusters and in the cen- ter the elements of the selected clusters. By dragging and dropping cluster elements, they can be assigned to other clusters or deleted. La- beled clusters and their elements are shown with the label color. When hovering an element with the mouse, a tooltip shows the full video frame with the gaze point and thumbnail border marked.", "DPI": 100, "CaptionBB": [28, 330, 378, 425], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 7, "Type": "Figure", "Imagebbox": [0.5178571428571429, 0.051414106145251395, 0.9655612244897959, 0.20261437908496732], "Caption": "Fig. 9: Annotation times for thirteen videos. The dynamic AOIs were la- beled directly in the video by drawing and tracking bounding shapes. Annotator 1 and 2 applied our approach. Their different completion times result from different strategies applied to solve the task.", "DPI": 100, "CaptionBB": [405, 225, 755, 280], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 8, "Page": 7, "Type": "Figure", "Imagebbox": [0.08587786259541985, 0.04862081005586592, 0.49427480916030536, 0.2371682960893855], "Caption": "Fig. 9: Annotation times for thirteen videos. The dynamic AOIs were la- beled directly in the video by drawing and tracking bounding shapes. Annotator 1 and 2 applied our approach. Their different completion times result from different strategies applied to solve the task.", "DPI": 100, "CaptionBB": [405, 225, 755, 280], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 9, "Page": 7, "Type": "Figure", "Imagebbox": [0.5190839694656488, 0.2762744413407821, 0.9618320610687023, 0.47180516759776536], "Caption": "Fig. 9: Annotation times for thirteen videos. The dynamic AOIs were la- beled directly in the video by drawing and tracking bounding shapes. Annotator 1 and 2 applied our approach. Their different completion times result from different strategies applied to solve the task.", "DPI": 100, "CaptionBB": [405, 225, 755, 280], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 10, "Page": 8, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.29971988795518206, 0.9502551020408163, 0.353874883286648], "Caption": "Fig. 13: Timeline overview showing the longest relative time interval when two participants looked at the same AOI.", "DPI": 100, "CaptionBB": [103, 387, 667, 403], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 11, "Page": 8, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.20354808590102708, 0.9502551020408163, 0.2642390289449113], "Caption": "Fig. 12: Scarf plot of the data that was analyzed in the expert user study. A yellow segment (marked orange) was misclassified by two participants.", "DPI": 100, "CaptionBB": [28, 291, 742, 307], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 12, "Page": 8, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.05001745810055866, 0.9502551020408163, 0.1503267973856209], "Caption": "Fig. 11: Scarf plot of 13 participants labeled with our approach. Black areas depict segments that were put to the garbage can due to gaze points outside the AOIs.", "DPI": 100, "CaptionBB": [28, 169, 742, 198], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 13, "Page": 9, "Type": null, "Imagebbox": [0.06297709923664122, 0.08214036312849161, 0.48664122137404575, 0.19806215083798884], "Caption": null, "DPI": null, "CaptionBB": null, "first_confirmed": false, "second_confirmed": false}]