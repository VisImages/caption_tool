[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.07487309644670051, 0.16267482517482518, 0.9390862944162437, 0.5162790697674419], "Caption": "Fig. 1. A scalable MPI visualization. Rather than plotting MPI calls per process, this view plots the duration of the call on the y-axis with a log scale versus time on the x-axis. Patterns such as simultaneous start/end times, clusters and trends of similar calls, and particularly long communications can be seen. The data was collected from running matrix operations from the ScaLAPACK library on 256 nodes of NERSC\u2019s Franklin supercomputer.", "DPI": 100, "CaptionBB": [75, 569, 724, 622], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 3, "Type": null, "Imagebbox": [0.05152671755725191, 0.06197552447552448, 0.5038167938931297, 0.19484265734265735], "Caption": null, "DPI": null, "CaptionBB": null, "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 3, "Type": null, "Imagebbox": [0.08969465648854962, 0.3179195804195804, 0.4618320610687023, 0.4577797202797203], "Caption": null, "DPI": null, "CaptionBB": null, "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 4, "Type": "Figure", "Imagebbox": [0.025380710659898477, 0.05777972027972028, 0.49236641221374045, 0.6111627906976744], "Caption": "Fig. 4. MPI Call Plots. Different representations of MPI calls for direct visualization. Arches (a) are easier to visually follow, while lines (b) and start/end points (c) are more scalable with respect to screen space. In each case, one call is selected, with details presented in text form, and with all calls from the same process highlighted.", "DPI": 100, "CaptionBB": [29, 674, 379, 741], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 4, "Type": "Figure", "Imagebbox": [0.4987309644670051, 0.054982517482517486, 0.9467005076142132, 0.6120930232558139], "Caption": "Fig. 5. Opacity Scaling. Standard opacity accumulation (a) has trouble both with too much overplotting and with outliers being too transparent. Applying a minimum value and scaling the opacity (b) helps, and apply- ing a logarithmic scale (c) helps even more.", "DPI": 100, "CaptionBB": [394, 675, 744, 728], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 6, "Type": null, "Imagebbox": [0.03435114503816794, 0.06617132867132867, 0.9809160305343512, 0.902534965034965], "Caption": null, "DPI": null, "CaptionBB": null, "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 7, "Type": "Figure", "Imagebbox": [0.07106598984771574, 0.061395348837209304, 0.9708121827411168, 0.4316279069767442], "Caption": "Fig. 7. Effect of scale on matrix multiplication. As the number of processes increases, so does the cost of setting up the communication structures before actually executing the matrix operation. Overall time for running the program increases with the number of processes and scale of the data, but the multiplication takes less time per element. The ef\ufb01ciency of the system drops substantially with large numbers of processes.", "DPI": 100, "CaptionBB": [42, 481, 757, 521], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 8, "Type": "Figure", "Imagebbox": [0.025380710659898477, 0.054982517482517486, 0.9479695431472082, 0.2762790697674419], "Caption": "Fig. 8. Running on 16,384 processes: At this scale, the matrix multiplication looks similar to Figure 6(b), with some differences. There is a clear section early in the operation with longer MPI calls caused by interference from the previous operation, as seen in the upper left of (a). When zoomed in further, some detailed trends can be seen in the midst of the ocean of communication, but these clusters are generally small, indicating good division of processes.", "DPI": 100, "CaptionBB": [29, 313, 744, 366], "first_confirmed": false, "second_confirmed": false}]