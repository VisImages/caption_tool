[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.09183673469387756, 0.12281007397082425, 0.923469387755102, 0.45769379490105677], "Caption": "Fig. 1. DGMTracker, a visual analytics tool that helps experts understand and diagnose the training processes of deep generative models (DGMs): (a) the loss changes; (b) the data flow visualization to illustrate how data flows through a DGM and disclose how other neurons influence the output of the neuron of interest; (c) visualization of the training dynamics (e.g., activation changes).", "DPI": 100, "CaptionBB": [73, 494, 722, 536], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 3, "Type": "Figure", "Imagebbox": [0.07397959183673469, 0.05228758169934641, 0.4642857142857143, 0.15219421101774042], "Caption": "Fig. 2. An example architecture of a GAN.", "DPI": 100, "CaptionBB": [40, 170, 247, 186], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 4, "Type": "Figure", "Imagebbox": [0.514030612244898, 0.8506069094304388, 0.9349489795918368, 0.9122315592903828], "Caption": "Fig. 5. Example patterns of data flow within each node: (a) abrupt changes of many activations; (b) abrupt changes of a few activations; (c) activations that become unstable after the focus snapshot.", "DPI": 100, "CaptionBB": [392, 979, 742, 1021], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 4, "Type": "Figure", "Imagebbox": [0.03443877551020408, 0.7956782910250878, 0.48596938775510207, 0.9084967320261437], "Caption": "Fig. 4. A hybrid visualization to illustrate the data flow at the snapshot level: (a) a DAG layout to visualize the structure of a DGM; (b) line charts to represent the data flow.", "DPI": 100, "CaptionBB": [28, 979, 378, 1021], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 4, "Type": "Figure", "Imagebbox": [0.1292372881355932, 0.051492244513459906, 0.8538135593220338, 0.2222222222222222], "Caption": "Fig. 3. DGMTracker consists of two modules: a data flow visualization and a training dynamics analysis. These modules are well aligned with the typical analytical process of an expert.", "DPI": 100, "CaptionBB": [28, 241, 742, 270], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 5, "Type": "Figure", "Imagebbox": [0.10714285714285714, 0.8297868181568707, 0.3983050847457627, 0.9057558104049328], "Caption": "Fig. 6. Illustration of the forward and backward contribution. The forward of the second feature map in the previous layer. This phenomenon (backward) contribution discloses how neurons are influenced by the cannot be identified when using K-Means clustering (Fig. 7(d)) because neurons in the previous (next) layer. the neurons are placed randomly.", "DPI": 100, "CaptionBB": [40, 975, 755, 1017], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 5, "Type": "Figure", "Imagebbox": [0.5178571428571429, 0.04948646125116713, 0.9642857142857143, 0.23622782446311857], "Caption": "Fig. 7. Credit visualization: (a) before clustering; (b) after K-Means clustering; (c) after organizing neurons as feature maps; (d) and (e) detail contribution when mouse hovers.", "DPI": 100, "CaptionBB": [405, 268, 755, 310], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 6, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.04994185691656068, 0.9502551020408163, 0.14098972922502334], "Caption": "Fig. 8. Comparison of different sampling methods: (a) without sampling; (b) random sampling; (c) blue-noise polyline sampling. The blue-noise polyline sampling algorithm can better reduce visual clutter (the red rectangle) and preserve outliers (the green rectangle).", "DPI": 100, "CaptionBB": [28, 155, 742, 184], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 8, "Page": 7, "Type": "Figure", "Imagebbox": [0.05612244897959184, 0.8179271708683473, 0.49489795918367346, 0.8636788048552755], "Caption": "Fig. 10. The network structure of the GAN used in the case study.", "DPI": 100, "CaptionBB": [40, 933, 363, 949], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 9, "Page": 7, "Type": "Figure", "Imagebbox": [0.5599489795918368, 0.04948646125116713, 0.9196428571428571, 0.10644257703081232], "Caption": "Fig. 11. The discriminator loss quickly stops changing after a few itera- tions which indicates the training process gets stuck.", "DPI": 100, "CaptionBB": [405, 118, 757, 147], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 10, "Page": 7, "Type": "Figure", "Imagebbox": [0.5153061224489796, 0.3786240274591963, 0.9681122448979592, 0.42048449257547543], "Caption": "Fig. 12. The gradients vanish when using an inappropriate loss.", "DPI": 100, "CaptionBB": [405, 455, 718, 471], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 11, "Page": 7, "Type": "Figure", "Imagebbox": [0.5153061224489796, 0.7460658879243126, 0.9655612244897959, 0.903828197945845], "Caption": "Fig. 13. The output changes of the discriminator: (a) the outputs invoked by generated images become close to 0; and (b) the outputs invoked by real images become close to 1.", "DPI": 100, "CaptionBB": [405, 975, 755, 1017], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 12, "Page": 7, "Type": "Figure", "Imagebbox": [0.2860169491525424, 0.054593019707258356, 0.5, 0.1925775158312894], "Caption": "Fig. 9. The pop-up menu for the iteration 8 where the training had been stuck and checked the data flow training dynamics visualization. of gradients at the snapshot level. He found that the gradients were", "DPI": 100, "CaptionBB": [224, 209, 755, 240], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 13, "Page": 8, "Type": "Figure", "Imagebbox": [0.5021186440677966, 0.04684108172276223, 0.9502551020408163, 0.15779645191409897], "Caption": "Fig. 17. The changes of (wt+1 i \u2212 wti )gti in the training processes: (a) using Adam; (b) using RMSprop.", "DPI": 100, "CaptionBB": [392, 181, 742, 206], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 14, "Page": 8, "Type": "Figure", "Imagebbox": [0.03389830508474576, 0.21893410497857618, 0.4894067796610169, 0.5104069731956304], "Caption": "Fig. 17. The changes of (wt+1 i \u2212 wti )gti in the training processes: (a) using Adam; (b) using RMSprop.", "DPI": 100, "CaptionBB": [392, 181, 742, 206], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 15, "Page": 8, "Type": "Figure", "Imagebbox": [0.03389830508474576, 0.8390891437382662, 0.4851694915254237, 0.9135077483894289], "Caption": "Fig. 17. The changes of (wt+1 i \u2212 wti )gti in the training processes: (a) using Adam; (b) using RMSprop.", "DPI": 100, "CaptionBB": [392, 181, 742, 206], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 16, "Page": 9, "Type": "Figure", "Imagebbox": [0.05102040816326531, 0.05042016806722689, 0.4936440677966102, 0.1507170507150103], "Caption": "a training process. For example, the training process of the VAE Fig. 18. Comparison between using logarithmic variance and variance in we used generates more than 5TB data (250MB per snapshot and Gaussian sampling: (a) logarithmic variance; (b) variance. more than 20,000 snapshots). Storing all this data to the hard disk is", "DPI": 100, "CaptionBB": [40, 153, 755, 192], "first_confirmed": false, "second_confirmed": false}]