[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.12823529411764706, 0.1575405604719764, 0.8705882352941177, 0.4527272727272727], "Caption": "Figure 1: The C2 A system includes (A) Timeline Filtering View for selecting datasets within a specified time interval, (B) Aggregated Textual Information for displaying a textual summary of the crowd statistics and application specific performance, (C) Similarity View for displaying the overlap and Euclidean distance metric of the crowd demographics and video segment statistics, (D) Consensus Map for displaying the crowd consensus on polyp and polyp-free (benign) video segments along with aggregated crowd accuracy and timing, (E) Crowd View for displaying crowd demographics and rewards, (F) Video Segments View for displaying selected video segments, and (G) Word Cloud displaying keywords from user comments.", "DPI": 100, "CaptionBB": [106, 501, 744, 586], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 2, "Type": "Figure", "Imagebbox": [0.09294117647058824, 0.06363636363636363, 0.47058823529411764, 0.20636363636363636], "Caption": "Figure 2: (a) An existing workflow for medical crowdsourcing [23] and (b) our workflow. First, input data (colon video segments, in our case) are created and (A) presented to the crowd (non-expert users) and the clinical experts (radiologists in our case). The crowd (B) and the clinical experts (C) conduct micro-tasks (identifying each video segment as a polyp/polyp-free video segment, in our case), simultaneously. Once the results from the crowd and the radiologists are obtained, in the existing workflow [23], analysts perform sta- tistical analysis based on the selected criteria (D). However, in our workflow, analysts (clinical technicians) can interactively explore, analyze, and compare the results from the crowd with the ground truth from the medical experts using C2 A (E) and (F).", "DPI": 100, "CaptionBB": [73, 226, 411, 391], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 2, "Type": "Figure", "Imagebbox": [0.5870588235294117, 0.06818181818181818, 0.8552941176470589, 0.2081818181818182], "Caption": "Figure 3: The (a) supine and (b) prone colons, and the corresponding computed centerlines (shown in pink) for creating the virtual fly- throughs in VC, from rectum to cecum (antegrade direction) and cecum to rectum (retrograde direction).", "DPI": 100, "CaptionBB": [440, 228, 777, 283], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 4, "Type": "Table", "Imagebbox": [0.5164705882352941, 0.09363636363636364, 0.9364705882352942, 0.34363636363636363], "Caption": "Table 1: The consensus map can visualize two types of information with the corresponding elements and the aggregated information.", "DPI": 100, "CaptionBB": [439, 74, 775, 101], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 5, "Type": "Figure", "Imagebbox": [0.08588235294117647, 0.45181818181818184, 0.4788235294117647, 0.6672727272727272], "Caption": "Figure 5: Each cell of the matrix visualization in the consensus map displays the registered user response (if any), and the corresponding row and column represents a user and a video segment, respectively. We place a consensus slider on the top right corner of the map for selecting an appropriate threshold to obtain a consensus. On the left and top sides of the map, we also visualize, as enumerated in Table 1, (a) User aggregated information, (b) Aggregated user time to complete a task (20 video segments) normalized using time across all users, (c) Video segment aggregated information, (d) Aggregated time of users to complete a video segment normalized using time across all video segments, and (e) the direction of fly-through, such as antegrade and retrograde.", "DPI": 100, "CaptionBB": [72, 751, 410, 916], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 5, "Type": "Figure", "Imagebbox": [0.09294117647058824, 0.07545454545454545, 0.9047058823529411, 0.42], "Caption": "Figure 4: Two types of information in our consensus map view: (a) user response, and (b) statistics.", "DPI": 100, "CaptionBB": [175, 463, 673, 476], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 6, "Type": "Figure", "Imagebbox": [0.09294117647058824, 0.32545454545454544, 0.4717647058823529, 0.47], "Caption": "Figure 7: An example of our similarity view with predefined weights to cluster users based on the \u201cage\u201d parameter using (a) t-SNE, and (b) MDS.", "DPI": 100, "CaptionBB": [73, 520, 409, 561], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 6, "Type": "Figure", "Imagebbox": [0.12588235294117647, 0.06636363636363636, 0.8717647058823529, 0.25727272727272726], "Caption": "Figure 6: Illustration of our similarity view: (a) a glyph for each user and video segment used in our similarity view, (b) a similarity view based on users\u2019 demographics and rewards, and (c) a similarity view based on users\u2019 accuracy and timing on each video segment. In (a), the lightness of a circle indicates the number of polyps, respectively, and the length of an arc on a circle represent aggregated time to complete the task/video segment.", "DPI": 100, "CaptionBB": [73, 282, 775, 337], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 8, "Page": 7, "Type": "Table", "Imagebbox": [0.13294117647058823, 0.3781818181818182, 0.43176470588235294, 0.45636363636363636], "Caption": "Table 2: Specificity and sensitivity for the crowd (based on the 50% crowd consensus rate) and a medical expert in each case. Note the mismatch for case 1 due to the VC application parameter setting.", "DPI": 100, "CaptionBB": [73, 370, 410, 411], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 9, "Page": 7, "Type": "Figure", "Imagebbox": [0.2023529411764706, 0.22454545454545455, 0.36470588235294116, 0.3018181818181818], "Caption": "Figure 9: Our word cloud view for higher FOV and higher speed dataset.", "DPI": 100, "CaptionBB": [73, 332, 408, 359], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 10, "Page": 7, "Type": "Figure", "Imagebbox": [0.08588235294117647, 0.06636363636363636, 0.9141176470588235, 0.18181818181818182], "Caption": "Figure 8: Variations of our crowd view by focusing on different parameters such as (a) age, (b) gender, (c) education level, (d) medical expertise, (e) visualization expertise, and (f) rewards.", "DPI": 100, "CaptionBB": [73, 204, 777, 231], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 11, "Page": 8, "Type": "Figure", "Imagebbox": [0.09647058823529411, 0.3090909090909091, 0.9, 0.5527272727272727], "Caption": "Figure 11: Our consensus map shows polyp and polyp-free video segments in (a) higher FOV and speed dataset, and (b) lower FOV and speed dataset based on the 50% user consensus rate.", "DPI": 100, "CaptionBB": [73, 609, 775, 636], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 12, "Page": 8, "Type": "Figure", "Imagebbox": [0.11058823529411765, 0.06636363636363636, 0.8870588235294118, 0.2509090909090909], "Caption": "Figure 10: The effect of rewards in (a) higher FOV and speed dataset, and (b) lower FOV and speed dataset. In the consensus map view (A), users are sorted by accuracy and the demographics label (C) for each user shows its reward information, selected from the crowd view (D). In the similarity view (B), users are clustered based on the \u201crewards\u201d parameter by assigning predefined weights to parameters. Both views show that the rewards have no effect on the user accuracy or performance in both the datasets.", "DPI": 100, "CaptionBB": [73, 277, 777, 332], "first_confirmed": false, "second_confirmed": false}]