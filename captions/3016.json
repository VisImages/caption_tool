[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.05102040816326531, 0.15167493796526055, 0.9642857142857142, 0.4668534080298786], "Caption": "Fig. 1. The interface of the proposed system. During exploration, we can filter through a large number of sentence pairs summarized in (a). the current selected pair is displayed in (b). The model internal information (attention) is displayed in (d) and (e). The predicted probability (one of three labels: neural, entailment, contradiction) is shown in the barycentric coordinate in (f). Finally, the high-level model structure and the updates to the model are summarized in (c).", "DPI": 100, "CaptionBB": [73, 505, 722, 560], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 2, "Type": "Table", "Imagebbox": [0.5, 0.5676937441643324, 0.9681122448979592, 0.6665632754342432], "Caption": "Table 1. An illustration of natural language inference.", "DPI": 100, "CaptionBB": [438, 587, 698, 603], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 3, "Type": "Figure", "Imagebbox": [0.04974489795918367, 0.1975806451612903, 0.5016949152542373, 0.23902894491129786], "Caption": "Fig. 2. The shared structure of end-to-end NLP neural network models. in the input contribute most to the predicted label (often referred to", "DPI": 100, "CaptionBB": [40, 255, 755, 272], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 3, "Type": "Figure", "Imagebbox": [0.04974489795918367, 0.624379652605459, 0.48983050847457626, 0.7180205415499533], "Caption": "Fig. 3. Attention can be naturally explained as a form of alignment that ity (i.e., build the meaning of a sentence from the meaning of words exposes an interpretable layer in end-to-end neural networks. On the and phrases) exhibited in neural network linguistic models is examined left, the matrix shows the attention between the premise and hypothesis by utilizing techniques inspired by model interpretability techniques in pair. On the right, we illustrate that the attention can also be interpreted computer vision by Li et al. [11]. In the RNNVis [19] work, Ming et al as alignment. visualize the hidden state units in a recurrent neural network based on", "DPI": 100, "CaptionBB": [40, 768, 755, 842], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 4, "Type": "Figure", "Imagebbox": [0.5, 0.11017740429505135, 0.9502551020408163, 0.26517273576097106], "Caption": "Fig. 4. Perturbation-driven exploration of the natural language inference model. In the proposed tool, we enable the interrogation of the relation- ship between different components of the model via the perturbation- based analysis. The user can perturb the input sentences (i.e., replace words with synonymous), perturb the attention (i.e., alter the soft align- ment between sentences), and perturb the prediction (i.e., adjust the prediction by making updates to the model).", "DPI": 100, "CaptionBB": [392, 290, 744, 385], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 5, "Type": "Figure", "Imagebbox": [0.5153061224489796, 0.6876550868486352, 0.9668367346938775, 0.8683473389355743], "Caption": "Fig. 6. In the prediction view, the prediction is encoded as a point in the barycentric coordinate system of the triangle shown in (a). A density contour of the prediction is computed via kernel density estimation to emphasize the highly cluttered areas and distinguish the outliers. As illustrated in (b), the predicted label does not match the ground truth. Therefore, we apply a label reassignment operation, which triggers a model update.", "DPI": 100, "CaptionBB": [405, 931, 757, 1026], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 5, "Type": "Figure", "Imagebbox": [0.05102040816326531, 0.04001240694789082, 0.9655612244897958, 0.42016806722689076], "Caption": "Fig. 5. Attention visualization. In the graph attention view (a), a bipartite graph encoding is adopted, in which the edge thickness corresponds to the attention value. In the matrix attention view (b), the entries of ith row represent the probabilities of words in hypotheses align to the ith word in the premise. The user can alter the attention values via the pop-up interface illustrated in (c). We overlay the dependency tree (a1 ) grammar structure to highlight important words and simplify complex sentence to reduce clutter (d-e). In (f), we show the difference between two attention matrix (the comparison feature is controlled by the buttons in the top left of all attention plots (a2 ) ).", "DPI": 100, "CaptionBB": [40, 454, 755, 523], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 6, "Type": "Figure", "Imagebbox": [0.5135593220338983, 0.18889578163771711, 0.923469387755102, 0.415499533146592], "Caption": "Fig. 8. We summarized all prediction results of 10k sentence pairs in (a). The green block indicates correct predictions, the orange block indicates wrong predictions. The user can click on a treemap node to focus on the specific type of scenarios (e.g., E/E, indicating both the ground truth and the predicted label are Entailment) to automatically reveal the histogram (b) and scatterplot (c) for displaying the selected subset. The selection can be further narrowed down by selecting the bin in the histogram. In (c) and (d), each point corresponds to one sentence pair.", "DPI": 100, "CaptionBB": [392, 445, 744, 553], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 8, "Page": 6, "Type": "Figure", "Imagebbox": [0.04745762711864407, 0.5313275434243176, 0.47457627118644075, 0.7273573200992556], "Caption": "Fig. 8. We summarized all prediction results of 10k sentence pairs in (a). The green block indicates correct predictions, the orange block indicates wrong predictions. The user can click on a treemap node to focus on the specific type of scenarios (e.g., E/E, indicating both the ground truth and the predicted label are Entailment) to automatically reveal the histogram (b) and scatterplot (c) for displaying the selected subset. The selection can be further narrowed down by selecting the bin in the histogram. In (c) and (d), each point corresponds to one sentence pair.", "DPI": 100, "CaptionBB": [392, 445, 744, 553], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 9, "Page": 7, "Type": "Figure", "Imagebbox": [0.5220338983050847, 0.3415012406947891, 0.9681122448979592, 0.5098039215686274], "Caption": "Fig. 10. Editing the original attention (a) to correctly align the word \u201dheap\u201d with \u201dpile\u201d as shown in (b) (these two words are highlighted in orange). The change of attention leads to the change of prediction from neutral to the class boundary between neutral and entitlement.", "DPI": 100, "CaptionBB": [404, 545, 756, 600], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 10, "Page": 7, "Type": "Figure", "Imagebbox": [0.05423728813559322, 0.37127791563275436, 0.5050847457627119, 0.6045285359801489], "Caption": "Fig. 9. Prediction stability assessment. In (a)(b)(c), we estimate the over- experts want to know how the model arrives at a conclusion, and if the all prediction stability (regarding synonymous perturbation) for each type prediction is incorrect, where in the model the error occurs. Examining of prediction over the entire development set (10k examples). The user the decision-making process is not only instrumental in evaluating the can drill down to individual examples by using the interface described in model performance but also essential for hypothesizing improvement Fig. 8. In (d), we illustrate a highly unstable prediction, where the original strategies for future models. In the NLI model, the three stages (en-", "DPI": 100, "CaptionBB": [40, 643, 757, 715], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 11, "Page": 8, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.04001240694789082, 0.9502551020408163, 0.2408963585434174], "Caption": "Fig. 11. Experiment with all configurations for the label reassignment optimization. As shown in (d), the update to the attention stage seems to have significantly less impact on the prediction result compared to the classifier or encoder stage of the model.", "DPI": 100, "CaptionBB": [28, 263, 743, 292], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 12, "Page": 9, "Type": "Figure", "Imagebbox": [0.05102040816326531, 0.04249379652605459, 0.9655612244897958, 0.3062558356676004], "Caption": "Fig. 12. The dependency tree provides valuable information that can help fix the prediction error. In (a), the model mistakenly aligns the word green, which leads to an incorrect prediction. After examining the dependency tree (highlighted by pink squares), we can see the two greens are attached to different words. In (b), by editing the attention and forcing the alignment of the two greens to be zero, the prediction label is corrected to neutral.", "DPI": 100, "CaptionBB": [40, 329, 757, 371], "first_confirmed": false, "second_confirmed": false}]