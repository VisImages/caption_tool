[{"ImageID": 0, "Page": 1, "Type": "Figure", "Imagebbox": [0.09438775510204081, 0.12313895781637717, 0.9221938775510204, 0.3489454094292804], "Caption": "Fig. 1: Computing Cleveland and McGill\u2019s Position-Angle Experiment using Convolutional Neural Networks. We replicate the original experiment by asking CNNs to assess the relationship between values encoded in pie charts and bar charts. We find that CNNs can predict quantities more accurately from bar charts (mean squared error (MSE) in green).", "DPI": 100, "CaptionBB": [73, 374, 721, 419], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 1, "Page": 3, "Type": "Figure", "Imagebbox": [0.0548469387755102, 0.10550887021475257, 0.5016949152542373, 0.2075062034739454], "Caption": "cross-validation [49]. We run every experiment separately twelve Fig. 2: Network Architecture. We use a multilayer perceptron (MLP) times (four times for the \u2018from scratch\u2019 networks due to significantly- to perform linear regression for continuous variable output. We also longer training times), and randomly select (without replacement) learn convolutional features through LeNet (2 layers, filter size 5 \u00d7 5), the 60% of our data as training data, 20% as validation, and 20% as test. VGG19 (16 layers, filter size 3 \u00d7 3), or Xception (36 layers, filter size", "DPI": 100, "CaptionBB": [40, 359, 756, 433], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 2, "Page": 3, "Type": "Figure", "Imagebbox": [0.0576271186440678, 0.2161910669975186, 0.5033898305084745, 0.3377791563275434], "Caption": "cross-validation [49]. We run every experiment separately twelve Fig. 2: Network Architecture. We use a multilayer perceptron (MLP) times (four times for the \u2018from scratch\u2019 networks due to significantly- to perform linear regression for continuous variable output. We also longer training times), and randomly select (without replacement) learn convolutional features through LeNet (2 layers, filter size 5 \u00d7 5), the 60% of our data as training data, 20% as validation, and 20% as test. VGG19 (16 layers, filter size 3 \u00d7 3), or Xception (36 layers, filter size", "DPI": 100, "CaptionBB": [40, 359, 756, 433], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 3, "Page": 5, "Type": "Figure", "Imagebbox": [0.08305084745762711, 0.04855275443510738, 0.9196428571428571, 0.4295051353874883], "Caption": "Fig. 3: Elementary perceptual tasks results for the most complex task parameterization. In each column: Left: Example stimuli image. Right: MLAE and bootstrapped 95% confidence intervals for different networks. Lower MLAE scores are better. The * indicates fine-tuned ImageNet weights instead of weights trained from scratch. Bottom right shows \u2018multi\u2019 VGG19 and Xception networks trained on all perceptual tasks, combined with optional 9\u00d7 increase of training data.", "DPI": 100, "CaptionBB": [40, 468, 757, 527], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 4, "Page": 5, "Type": "Figure", "Imagebbox": [0.05084745762711865, 0.6132133995037221, 0.5016949152542373, 0.8166873449131513], "Caption": "Fig. 3: Elementary perceptual tasks results for the most complex task parameterization. In each column: Left: Example stimuli image. Right: MLAE and bootstrapped 95% confidence intervals for different networks. Lower MLAE scores are better. The * indicates fine-tuned ImageNet weights instead of weights trained from scratch. Bottom right shows \u2018multi\u2019 VGG19 and Xception networks trained on all perceptual tasks, combined with optional 9\u00d7 increase of training data.", "DPI": 100, "CaptionBB": [40, 468, 757, 527], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 5, "Page": 6, "Type": "Figure", "Imagebbox": [0.5, 0.3118580765639589, 0.9489795918367347, 0.5144724556489262], "Caption": "Fig. 6: Training efficiency of the position-angle experiment. Mean Squared Error (MSE) loss after each epoch during training, computed on previously-unseen validation data. We train all networks 12 times (4 times for VGG19 and Xception due to significantly longer training times). VGG19 * and Xception * use ImageNet weights. All networks reduce MSE loss faster when learning bar charts compared to pie charts.", "DPI": 100, "CaptionBB": [392, 558, 742, 645], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 6, "Page": 6, "Type": "Figure", "Imagebbox": [0.4987244897959184, 0.04497518610421836, 0.9502551020408163, 0.22128851540616246], "Caption": "Fig. 5: Computational results of the position-angle experiment. Left: Example stimuli. Right: MLAE and bootstrapped 95% con- fidence intervals (the lower, the better). VGG19 * and Xception * fine tune ImageNet weights, with all other networks trained from scratch. We show Cleveland and McGill\u2019s human results in black [10].", "DPI": 100, "CaptionBB": [392, 244, 745, 317], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 7, "Page": 6, "Type": "Figure", "Imagebbox": [0.059322033898305086, 0.04373449131513648, 0.48135593220338985, 0.8080024813895782], "Caption": "Fig. 6: Training efficiency of the position-angle experiment. Mean Squared Error (MSE) loss after each epoch during training, computed on previously-unseen validation data. We train all networks 12 times (4 times for VGG19 and Xception due to significantly longer training times). VGG19 * and Xception * use ImageNet weights. All networks reduce MSE loss faster when learning bar charts compared to pie charts.", "DPI": 100, "CaptionBB": [392, 558, 742, 645], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 8, "Page": 7, "Type": "Figure", "Imagebbox": [0.5420918367346939, 0.04855275443510738, 0.9426020408163265, 0.5648926237161531], "Caption": "Fig. 7: Computational results of the position-length experiment. Left: Type 1\u20135 stimuli for divided and grouped bar charts (as per Cleve- land and McGill). Right: MLAE and bootstrapped 95% confidence intervals of our networks. Star * denotes networks using ImageNet weights. The last row shows \u2018multi\u2019 networks trained on a random stream of types 1\u20135 (plus VGG19 with 5\u00d7 training data). We include human performance (black) from the original experiment (ClMcG, top) and from Heer and Bostock\u2019s crowdsourced studies (HeerBos, bottom).", "DPI": 100, "CaptionBB": [404, 612, 756, 726], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 9, "Page": 8, "Type": "Figure", "Imagebbox": [0.5016949152542373, 0.04621588089330025, 0.9502551020408163, 0.2913165266106443], "Caption": "Fig. 9: Computational results of the point cloud experiment. Left: We create 2D point clouds with 10, 100, and 1000 initial dots. Then, we add up to 10 new dots. For humans, it is possible to estimate how many dots are added if there are initially 10 points, but it is impossible to see how many dots are added when starting with 1000 dots. Right: We let our networks regress the number of added dots and report MLAE and bootstrapped 95% confidence intervals.", "DPI": 100, "CaptionBB": [392, 321, 744, 421], "first_confirmed": false, "second_confirmed": false}, {"ImageID": 10, "Page": 8, "Type": "Figure", "Imagebbox": [0.03571428571428571, 0.04249379652605459, 0.47627118644067795, 0.2100840336134454], "Caption": "Fig. 8: Computational results of the bars-and-framed-rectangles experiment. Left: Stimuli of two bars for length judgment (bottom) following Cleveland and McGill\u2019s setting. Perceiving which bar is longer is significantly easier for humans when a frame is added (top). Right: For networks (trained from scratch, or * indicates ImageNet weights), there seems no significant difference between the encodings as reported by MLAE and bootstrapped 95% confidence intervals.", "DPI": 100, "CaptionBB": [27, 234, 379, 334], "first_confirmed": false, "second_confirmed": false}]